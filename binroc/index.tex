\documentclass[article]{jss}
\usepackage[utf8]{inputenc}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\author{
John Muschelli\\Department of Biostatistics, Johns Hopkins Bloomberg School of Public
Health
}
\title{ROC and AUC with a Binary Predictor: a Potentially Misleading Metric}

\Plainauthor{John Muschelli}
\Plaintitle{ROC and AUC with a Binary Predictor: a Potentially Misleading Metric}
\Shorttitle{Binary Predictor ROC}

\Abstract{
In analysis of binary outcomes, the receiver operator characteristic
(ROC) curve is heavily used to show the performance of a model or
algorithm. The ROC curve is informative about the performance over a
series of thresholds and can be summarized by the area under the curve
(AUC), a single number. When a \textbf{predictor} is categorical, the
ROC curve has only as many thresholds as the one less than number of
categories; when the predictor is binary there is only one threshold. As
the AUC may be used in decision-making processes on determining the best
model, it important to discuss how it agrees with the intuition from the
ROC curve. We discuss how the interpolation of the curve between
thresholds with binary predictors can largely change the AUC. Overall,
we believe a linear interpolation from the ROC curve with binary
predictors, which is most commonly done in software, can lead to
misleading results.
}

\Keywords{roc, auc, area under the curve, \proglang{R}}
\Plainkeywords{roc, auc, area under the curve, R}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
    John Muschelli\\
  Department of Biostatistics, Johns Hopkins Bloomberg School of Public
  Health\\
  615 N Wolfe St Baltimore, MD 21205\\
  E-mail: \email{jmuschel@jhsph.edu}\\
  URL: \url{http://johnmuschelli.com}\\~\\
  }

% Pandoc header
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

\usepackage{amsmath} \usepackage{subfig}

\begin{document}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In many applications, receiver operator characteristic (ROC) curves are
used to show how a predictor compares to the true outcome. One of the
large advantages of ROC analysis is that it is threshold-agnostic;
performance of a predictor is estimated without a specific threshold and
also gives a criteria to choose an optimal threshold based on a certain
cost function or objective. Typically, an ROC analysis shows how
sensitivity (true positive rate) changes with varying specificity (true
negative rate or \(1 - \text{false positive rate}\)). Analyses also
typically weigh false positives and false negatives equally.

Many predictors, especially medical tests, result in a binary decision;
a value is higher than a pre-determined threshold or a substance is
present. Similarly, some predictors are simply categorical or discrete
such as low, normal, or high blood pressure while others are categorical
by nature such as having a specific gene or not. These are useful
indicators of presence a disease, the primary outcome of interest. The
predictive capabilities of the variable is commonly summarized by the
area under the curve (AUC). Additionally, partial ROC (pROC) analysis
keeps a specificity fixed and can summarize a predictor by the partial
AUC (pAUC) or the optimal sensitivity at that fixed false positive rate.

If one assumes the binary predictor is generated from a continuous
distribution that has been thresholded, then the sensitivity of this
thresholded predictor actually represents one point on the ROC curve for
the underlying continuous value. Therefore the ROC curve of a binary
predictor is not really appropriate, but should be represented by a
single point on the curve. But alas, ROC and AUC analysis is done on
binary predictors and used to inform if one variable is more predictive
than the other
\citep{jama, jama2, glaveckaite2011value, blumberg2016technology, budwega2016factors, mwipatayi}.
For example, these cases show that researchers use ROC curves and AUC to
evaluate predictors, even when the predictors are categorical or binary.
Although there is nothing inherently wrong with this comparison, it can
lead to drastically different predictors being selected based on these
criteria if ties are treated slightly different ways.

A more appropriate comparison of a continuous predictor and the binary
predictor may be to compare the sensitivity and specificity (or overall
accuracy) of the continuous predictor given the optimal threshold versus
that of the binary predictor.

\citet{fawcett2006introduction} describes how ties are handled in a
predictor. Ties are distinctly relevant for discrete and binary
predictors or models that predict a discrete number of values, where
many observations can have the same value/risk score. When drawing the
ROC curve, one can assume that all the ties do not correctly classify
the outcome (Fawcett called the ``pessimistic'' approach) or that all
the ties do correctly classify the outcome (called the ``optimistic''
approach), see Fig. 6 in \citep{fawcett2006introduction}. But Fawcett
notes (emphasis in original):

\begin{quote}
Any mixed ordering of the instances will give a different set of step
segments within the rectangle formed by these two extremes. However, the
ROC curve should represent the \emph{expected} performance of the
classifier, which, lacking any other information, is the average of the
pessimistic and optimistic segments.
\end{quote}

This ``expected'' performance directly applies to the assignment of a
half probability of success when the data are tied, which is implied by
the ``trapezoidal rule'' from \citet{hanley1982meaning}.
\citet{fawcett2006introduction} also states in the calculation of AUC
that ``trapezoids are used rather than rectangles in order to average
the effect between points''. This trapezoidal rule applies additional
areas to the AUC based on ties of the predictor, giving a half a
probability. This addition of half probability is linked to how ties are
treated in the Wilcoxon rank sum test. As much of the theory of ROC
curve testing, and therefore testing of differences in AUC, is based on
the theory of the Wilcoxon rank-sum test, this treatment of ties is
relevant to statistical inference.

Others have discussed insights into binary predictors in addition to
\citet{fawcett2006introduction}, but they are mentioned in small
sections of the paper \citep{saito2015precision, pepe2009estimation}.
Other information regarding ties and binary data are blog posts or
working papers such as
\url{http://blog.revolutionanalytics.com/2016/11/calculating-auc.html}
or \url{https://www.epeter-stats.de/roc-curves-and-ties/}, which was
written by the author of the \pkg{fbroc} \citep{fbroc} package, which we
will discuss below. Most notably, \citet{hsu2014inference} is an
extensive discussion of ties, but the paper was not published.

Although many discuss the properties of ROC and AUC analyses, we wish to
plainly show the math and calculations of the AUC with a binary
predictor explore commonly-used statistical software for ROC curve
creation and AUC calculation in a variety of packages and languages.
Overall, we believe that AUC calculations alone may be misleading for
binary or categorical predictors depending on the definition of the AUC.
We propose to be explicit when reporting the AUC in terms of the
approach to ties.

\hypertarget{mathematical-proof-of-auc-for-single-binary-predictor}{%
\section{Mathematical Proof of AUC for Single Binary
Predictor}\label{mathematical-proof-of-auc-for-single-binary-predictor}}

First, we will show how the AUC is defined in terms of probability. This
representation is helpful in discussing the disconnect between the
stated interpretation of the AUC, the formal definition, and how the
treatment of ties is crucial when the data are discrete. Let us assume
we have a binary predictor \(X\) and a binary outcome \(Y\), such that
\(X\) and \(Y\) only take the values \(0\) and 1, the number of
replicates is not relevant here. Let \(X_{i}\) be the values of
\(X | Y = i\), where \(i \in \{0, 1\}\).

\citet{fawcett2006introduction} goes on to state:

\begin{quote}
AUC of a classifier is equivalent to the probability that the classifier
will rank a randomly chosen positive instance higher than a randomly
chosen negative instance.
\end{quote}

In other words, we could discern the definition
\(\text{AUC} = P(X_{1} > X_{0})\). As there are only two outcomes for
\(X\), we can expand this probability using the law of total
probability:

\begin{align}
P(X_{1} > X_{0}) &= P(X_{1} > X_{0} | X_{1} = 1) P(X_{1} = 1) \nonumber \\
&+ P(X_{1} > X_{0} | X_{1} = 0) P(X_{1} = 0) \label{eq:expand1} \\
&= P(X_{1} > X_{0} | X_{1} = 1) P(X_{1} = 1) \label{eq:expand}
\end{align}

as \(P(X_{1} > X_{0} | X_{1} = 0) = 0\) because \(X_{0} \in \{0, 1\}\).
We see that \(P(X_{1} = 1)\) in equation \eqref{eq:expand} is the
sensitivity:

\begin{align*}
P(X_{1} = 1) &= P(X = 1 | Y = 1)\\
&= \frac{TP}{TP + FN} \\
&= \text{sensitivity}
\end{align*}

and that \(P(X_{1} > X_{0} | X_{1} = 1)\) in equation \eqref{eq:expand}
is the specificity:

\begin{align*}
P(X_{1} > X_{0} | X_{1} = 1) &= P(X_{1} > X_{0} | X_{1} = 1, X_{0} =1) P(X_{0} = 1) \\
&+ P(X_{1} > X_{0} | X_{1} = 1, X_{0} =0) P(X_{0} = 0) \\
&= P(X_{1} > X_{0} | X_{1} = 1, X_{0} =0) P(X_{0} = 0) \\
&= P(X_{0} = 0) \\
&= P(X = 0 | Y = 0)\\
&= \frac{TN}{TN + FP} \\
&= \text{specificity}
\end{align*}

Therefore, we combine these two to show that equation \eqref{eq:expand}
reduces to:

\[
P(X_{1} > X_{0}) = \text{specificity} \times \text{sensitivity}
\]

Using the definition of AUC as \(P(X_{1} > X_{0})\), it is the
sensitivity times the specificity.

If we change the definition of AUC slightly while accounting for ties,
which we call \(\text{AUC}_{\text{w/ties}}\), to \[
\text{AUC}_{\text{w/ties}} = P(X_{1} > X_{0}) + \frac{1}{2} P(X_{1} = X_{0})
\] This AUC is the one reported by most software, as we will see below.

\hypertarget{simple-concrete-example}{%
\subsection{Simple Concrete Example}\label{simple-concrete-example}}

To give some intuition of this scenario, we will assume \(X\) and \(Y\)
have the following joint distribution, where \(X\) is along the rows and
\(Y\) is along the columns:

\begin{table}

\caption{\label{tab:create_tab}A simple 2x2 table of a binary predictor (rows) versus a binary outcome (columns)}
\centering
\begin{tabular}[t]{l|r|r}
\hline
  & 0 & 1\\
\hline
0 & 52 & 35\\
\hline
1 & 32 & 50\\
\hline
\end{tabular}
\end{table}

Therefore, the AUC should be equal to
\(\frac{50}{85} \times \frac{52}{84}\), which equals 0.364.

We will define this as the the strict definition of AUC, where ties are
not taken into account and we are using strictly greater than in the
probability and will call this value \(\text{AUC}_{\text{definition}}\).

Note, if we reverse the labels, then the sensitivity and the specificity
are estimated by \(1\) minus that measure, or
\(\frac{35}{85} \times \frac{32}{84}\), which is equal to 0.157.

Thus, as this AUC is less than the original labeling, we would choose
that with the original labeling.

If we used the calculation for \(\text{AUC}_{\text{w/ties}}\) we see
that we estimate AUC by
\(\text{AUC}_{\text{definition}} + \frac{1}{2}\left( \frac{50 + 52}{169}\right)\),
which is equal to 0.604.

\hypertarget{monte-carlo-estimation-of-auc}{%
\subsubsection{Monte Carlo Estimation of
AUC}\label{monte-carlo-estimation-of-auc}}

We can also show that if we use simple Monte Carlo sampling, we can
randomly choose \(X_{0}\) and \(X_{1}\). From these samples, we can
estimate these AUC based on the definitions above. Here, the function
\code{est.auc} samples \(10^{6}\) random samples from \(X_{1}\) and
\(X_{0}\), then calculates \(\widehat{\text{AUC}}_{\text{definition}}\)
and \(\widehat{\text{AUC}}_{\text{w/ties}}\):

\begin{CodeChunk}

\begin{CodeInput}
R> est.auc = function(x, y, n = 1000000) {
R+   x1 = x[y == 1] # sample x | y = 1
R+   x0 = x[y == 0] # sample x | y = 0
R+   c1 = sample(x1, size = n, replace = TRUE)
R+   c0 = sample(x0, size = n, replace = TRUE)
R+   auc.defn = mean(c1 > c0) # compare
R+   auc.wties = auc.defn + 1/2 * mean(c1 == c0) # compare
R+   return(c(auc.definition = auc.defn,
R+            auc.wties = auc.wties))
R+ }
R> sample.estauc = est.auc(x, y)
R> sample.estauc
\end{CodeInput}

\begin{CodeOutput}
auc.definition      auc.wties 
      0.364631       0.603990 
\end{CodeOutput}
\end{CodeChunk}

And thus we see these simulations agree with the values estimated above.

\hypertarget{geometric-argument-of-auc}{%
\subsubsection{Geometric Argument of
AUC}\label{geometric-argument-of-auc}}

We will present a geometric discussion of the ROC as well. In Figure
\ref{fig:main}, we show the ROC curve for the simple concrete example.
In panel A, we show the point of sensitivity/specificity connected by
the step function, and the associated AUC is represented in the shaded
blue area, representing \(\text{AUC}_{\text{definition}}\). In panel B,
we show the additional shaded areas that are due to ties in orange and
red; all shaded areas represent \(\text{AUC}_{\text{w/ties}}\). We can
see this by expanding \(P(X_{1} = X_{0})\) such that:

\begin{align*}
P(X_{1} = X_{0}) &= P(X_{1} = 1, X_{0} = 1) + P(X_{1} = 0, X_{0} = 0) \\
&= P(X_{1} = 1) P(X_{0} = 1) + P(X_{1} = 0) P(X_{0} = 0) \\
&= \left(\text{sensitivity} \times (1 - \text{specificity})\right) + \left((1- \text{sensitivity}) \times \text{specificity}\right)
\end{align*}

so that we have

\begin{align*}
\text{AUC}_{\text{w/ties}} &= \text{specificity} \times \text{sensitivity} \\
&+ \frac{1}{2} \left(\text{sensitivity} \times (1 - \text{specificity})\right) \\
&+ \frac{1}{2} \left((1- \text{sensitivity}) \times \text{specificity}\right)
\end{align*}

Thus, we can see that geometrically from Figure \ref{fig:main}:

\begin{align*}
\text{AUC}_{\text{w/ties}} &= \includegraphics[width=1.5in,keepaspectratio]{addition_small.png}
\end{align*}

\begin{CodeChunk}
\begin{figure}[h]

{\centering \includegraphics{index_files/figure-latex/main-1} 

}

\caption[ROC curve of the data in the simple concrete example]{ROC curve of the data in the simple concrete example.  Here we present a standard ROC curve, with the false positive rate or $1 - \text{specificity}$ on the x-axis and true positive rate or sensitivity on the y-axis.  The dotted line represents the identity. The shaded area in panel represents the AUC for the strict definition.  The additional shaded areas on panel B represent the AUC when accounting for ties.  }\label{fig:main}
\end{figure}
\end{CodeChunk}

\hypertarget{auc-calculation-in-statistical-software}{%
\subsection{AUC Calculation in Statistical
Software}\label{auc-calculation-in-statistical-software}}

We will explore the estimated ROC curve and AUC from the implementations
in the following \texttt{R} packages: \pkg{ROCR} \citep{ROCR},
\pkg{caTools} \citep{caTools}, \pkg{pROC} \citep{pROC}, and \pkg{fbroc}
\citep{fbroc}. We will also show these agree with the Python
implementation in \texttt{sklearn.metrics} from \pkg{scikit-learn}
\citep{scikitlearn}, the Stata functions \texttt{roctab} and
\texttt{rocreg} \citep{bamber1975area, delong}, and the SAS software
functions \texttt{proc\ logistic} with \texttt{roc} and
\texttt{roccontrast} . We note that these functions all count half the
probability of ties, which raises the AUC.

\hypertarget{auc-calculation-current-implementations}{%
\section{AUC Calculation: Current
Implementations}\label{auc-calculation-current-implementations}}

This section will present code and results from commonly-used
implementations of AUC estimation from R, Python, and Stata. We will
note agreement with the definitions of AUC above and any discrepancies.
This section is not to be exhaustive, but show that these definitions
are consistently used in AUC analysis, primarily
\(\widehat{\text{AUC}}_{\text{w/ties}}\).

\hypertarget{r}{%
\subsection{R}\label{r}}

Here we will show the AUC calculation from the common \texttt{R}
packages for ROC analysis. We will show that each report the value
calculated in \(\text{AUC}_{\text{w/ties}}\). The \pkg{caTools} package
calculates AUC using the \texttt{colAUC} function:

\begin{CodeChunk}

\begin{CodeInput}
R> library(caTools)
R> colAUC(x, y)
\end{CodeInput}

\begin{CodeOutput}
             [,1]
0 vs. 1 0.6036415
\end{CodeOutput}
\end{CodeChunk}

In \pkg{ROCR}, AUC is calculated from a \code{performance} object, which
takes in a \code{prediction} object:

\begin{CodeChunk}

\begin{CodeInput}
R> library(ROCR)
R> pred = prediction(x, y)
R> auc.est = performance(pred, "auc")
R> auc.est@y.values[[1]]
\end{CodeInput}

\begin{CodeOutput}
[1] 0.6036415
\end{CodeOutput}
\end{CodeChunk}

The \pkg{pROC} package calculates AUC using the \texttt{roc} function:

\begin{CodeChunk}

\begin{CodeInput}
R> library(pROC)
R> pROC.roc = pROC::roc(predictor = x, response = y)
R> pROC.roc[["auc"]]
\end{CodeInput}

\begin{CodeOutput}
Area under the curve: 0.6036
\end{CodeOutput}
\end{CodeChunk}

The \pkg{fbroc} package is one of the most popular packages for doing
ROC analysis \citep{fbroc}. Using the \texttt{fbroc::boot.roc} and
\texttt{fbroc::perf} functions, we have:

\begin{CodeChunk}

\begin{CodeInput}
R> library(fbroc)
R> fbroc.default = boot.roc(x, as.logical(y), 
R+                          n.boot = 1000, tie.strategy = 2)
R> auc.def = perf(fbroc.default, "auc")
R> auc.def[["Observed.Performance"]]
\end{CodeInput}

\begin{CodeOutput}
[1] 0.6036415
\end{CodeOutput}

\begin{CodeInput}
R> fbroc.alternative = boot.roc(x, as.logical(y), 
R+                              n.boot = 1000, tie.strategy = 1)
R> auc.alt = perf(fbroc.alternative, "auc")
R> auc.alt[["Observed.Performance"]]
\end{CodeInput}

\begin{CodeOutput}
[1] 0.6036415
\end{CodeOutput}
\end{CodeChunk}

which give identical results to above. Although these strategies for
ties are different, they are relevant for the plotting for the ROC
curve. The standard error calculation uses the second strategy (the
``pessimistic'' approach), which is described in a blog post
(\url{https://www.epeter-stats.de/roc-curves-and-ties/}) and can be seen
in Figure \ref{fig:rocs}C.

\hypertarget{sas-software}{%
\subsection{SAS Software}\label{sas-software}}

In SAS software (version 9.4 for Unix) \citep{sas}, let's assume we have
a data set named \texttt{roc} loaded with the variables/columns of
\texttt{x} and \texttt{y} as above. The following commands will produce
the ROC curve in figure

\begin{CodeChunk}

\begin{CodeInput}
R>   proc logistic data=roc;
R+       model y(event='1') = x;
R+       roc; roccontrast;
R+       run;      
\end{CodeInput}
\end{CodeChunk}

\url{https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm\#statug_logistic_sect040.htm}

\hypertarget{stata}{%
\subsection{Stata}\label{stata}}

In Stata (StatCorp, College Station, TX, version 13) \citep{stata}, the
function \texttt{roctab} is one common way to calculate an AUC:

\begin{CodeChunk}

\begin{CodeInput}
R> roctab x y
\end{CodeInput}


\begin{CodeOutput}
 . roctab x y

                      ROC                    -Asymptotic Normal--
           Obs       Area     Std. Err.      [95% Conf. Interval]
         --------------------------------------------------------
           169     0.6037       0.0379        0.52952     0.67793
\end{CodeOutput}
\end{CodeChunk}

which agrees with the calculation based on
\(\text{AUC}_{\text{w/ties}}\) and agrees with the estimates from above.
One can also calculate the AUC using the \texttt{rocreg} function:

\begin{CodeChunk}

\begin{CodeInput}
R> rocreg y x, nodots auc
\end{CodeInput}


\begin{CodeOutput}
 . rocreg y x, nodots auc

Bootstrap results                               Number of obs      =       169
                                                Replications       =      1000

Nonparametric ROC estimation

Control standardization: empirical
ROC method             : empirical

Area under the ROC curve

   Status    : y
   Classifier: x
------------------------------------------------------------------------------
             |    Observed               Bootstrap
         AUC |       Coef.       Bias    Std. Err.     [95% Conf. Interval]
-------------+----------------------------------------------------------------
             |    .3641457  -.0004513    .0451334     .2756857   .4526056  (N)
             |                                        .2771778    .452824  (P)
             |                                        .2769474   .4507576 (BC)
------------------------------------------------------------------------------
\end{CodeOutput}
\end{CodeChunk}

which agrees with the definition of \(\text{AUC}_{\text{definition}}\)
and is different from the output from \texttt{roctab}. The variance of
the estimate is based on a bootstrap estimate, but the point estimate
will remain the same regardless of using the bootstrap or not. This
disagreement of estimates is concerning as the reported estimated AUC
may be different depending on the command used in the estimation.

Using \texttt{rocregplot} after running this estimation, we see can
create an ROC curve, which is shown in Figure \ref{stata}. We see that
the estimated ROC curve coincides with the estimated AUC from
\texttt{rocreg} and the blue rectangle in Figure \ref{fig:main}.

\begin{figure}
     \centering
     \subfloat[][Stata ROC Plot ]{\includegraphics[width=0.32\linewidth]{stata_roc_cropped.png}\label{stata}}
     \subfloat[][Python ROC Plot from scikit-learn]{\includegraphics[width=0.32\linewidth]{python_roc_cropped.png}\label{python}} 
     \subfloat[][SAS ROC Plot ]{\includegraphics[width=0.32\linewidth]{sas_cropped.png}\label{sas}} \\
     \subfloat[][ROCR ROC plot ]{\includegraphics[width=0.32\linewidth]{ROCR.png}\label{rocr}}
     \subfloat[][pROC ROC plot ]{\includegraphics[width=0.32\linewidth]{pROC.png}\label{proc}}
     \subfloat[][fbroc Strategy 2 (default)
]{\includegraphics[width=0.32\linewidth]{fbroc2.png}\label{fbroc2}}
     \caption{Comparison of different ROC curves for different  \code{R} packages,  \code{scikit-learn} from  \code{Python},  \code{SAS}, and  \code{Stata}.  Each line represents the ROC curve, which corresponds to an according area under the curve (AUC).  The blue shading represents the confidence interval for the ROC curve in the  \code{fbroc} package.  Also, each software represents the curve as the false positive rate versus the true positive rate, though the  \code{pROC} package calls it sensitivity and specificity (with flipped axes).  Some put the identity line where others do not.  Overall the difference of note as to whether the ROC curve is represented by a step or a linear function. Using the first tie strategy for ties (non-default) in  \code{fbroc} gives the same confidence interval but an ROC curve using linear interpolation.}
     \label{fig:rocs}
\end{figure}

We see that all ROC curves are interpolated with a linear interpolation,
which coincides with the calculation based on
\(\text{AUC}_{\text{w/ties}}\), except for the Stata ROC curve, which
interpolates using a step function and coincides with
\(\text{AUC}_{\text{definition}}\). The confidence interval estimate of
the ROC curve for \texttt{fbroc}, which is shaded in blue in Figures
\ref{fbroc2}, corresponds to variability based on
\(\text{AUC}_{\text{definition}}\), though Figure \ref{fbroc1} shows the
ROC curve based on \(\text{AUC}_{\text{w/ties}}\).

\begin{figure}
     \centering
     \subfloat[][fbroc Strategy 1  ]{\includegraphics[width=0.48\linewidth]{fbroc1.png}\label{fig:fbroc1}}
     \subfloat[][fbroc Strategy 2 (default)
]{\includegraphics[width=0.48\linewidth]{fbroc2.png}\label{fig:fbroc2}}
     \caption{Comparison of different strategies for ties in the  \code{fbroc} package.  The blue shading represents the confidence interval for the ROC curve.  Overall the difference of note as to whether the ROC curve is represented by a step or a linear function. Using the first tie strategy for ties (non-default) in \code{fbroc} gives the same confidence interval as the second strategy but an ROC curve using linear interpolation, which may give an inconsistent combination of estimate and confidence interval.}
     \label{fig:fbrocs}
\end{figure}

In Figure \ref{fig:fbrocs} we show the differing plots for using the
first (panel \ref{fig:fbroc1}) and second (panel \ref{fig:fbroc2},
default, duplicated). We see that \texttt{fbroc} gives a different
estimate of AUC based on the tie strategy, but the same estimate of the
confidence interval of the ROC curve, regardless of tie strategy.
Therefore, there may give an inconsistent combination of estimate and
confidence interval.

\hypertarget{fawcett-example}{%
\subsection{Fawcett Example}\label{fawcett-example}}

In \citet{fawcett2006introduction}, the authors introduce a data set
where there there should be a completely correct classifier.

\begin{CodeChunk}

\begin{CodeInput}
R> faw = data.frame(y = c(rep(TRUE, 6), rep(FALSE, 4)),
R+                  x = c(0.99999, 0.99999, 0.99993, 
R+                        0.99986, 0.99964, 0.99955, 
R+                        0.68139, 0.50961, 0.48880, 0.44951))
R> faw = faw %>% mutate(hyp = x > 0.5)
R> pred = prediction(predictions = faw[, "x"], labels = faw[, "y"])
R> auc.estimated = performance(pred, "auc")
R> auc.estimated@y.values[[1]]
\end{CodeInput}

\begin{CodeOutput}
[1] 1
\end{CodeOutput}

\begin{CodeInput}
R> est.auc(x = faw[, "x"], y = faw[, "y"])
\end{CodeInput}

\begin{CodeOutput}
auc.definition      auc.wties 
             1              1 
\end{CodeOutput}
\end{CodeChunk}

\bibliography{binroc.bib}


\end{document}

