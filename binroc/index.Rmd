---
author:
  - name: John Muschelli
    affiliation: Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health
    address: >
      615 N Wolfe St
      Baltimore, MD 21205
    email: \email{jmuschel@jhsph.edu}
    url: http://johnmuschelli.
title:
  formatted: "ROC and AUC in R with a Binary Predictor"
  # If you use tex in the formatted title, also supply version without
  plain:     "ROC and AUC in R with a Binary Predictor"
  # For running headers, if needed
  short:     "Binary Predictor ROC"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [roc, auc, area under the curve, "\\proglang{R}"]
  plain:     [roc, auc, area under the curve, R]
preamble: >
  \usepackage{amsmath}
output: rticles::jss_article
bibliography: binroc.bib
---

# Introduction

In many applications, receiver operator characteristic (ROC) curves are used to show how a predictor compares to the true outcome.   One of the large draws of th ROC curve is that it is threshold-agnostic; it shows the performace of a predictor without a specific threshold and also gives a criteria to choose an optimal threshold based on a certain cost function.

Many predictors, especially medical tests, result in a binary decision; a value is higher than a pre-determined threshold or not.  Similarly, some are simply categorical or discrete; for example, blood pressure may be low, normal, or high.  These are useful indicators of presence disease, which is a primary outcome of interest.  

@fawcett2006introduction describes (in Fig. 6) how ties are handled in a predictor.  Ties are distinctly relevant for discrete and binary predictors or models that predict a discrete number of values, where many observations can have the same value/risk score.  When drawing the ROC curve, one can assume that all the ties do not correctly classify the outcome (Fawcett called the "pessimistic" approach) or that all the ties do correclty classify the outcome (called the "optimistic" approach).  But Fawcett notes: 
> Any mixed ordering of the instances will give a different set of step segments within the rectangle formed by these two extremes. However, the ROC curve should represent the *expected* performance of the classifier, which, lacking any other information, is the average of the pessimistic and optimistic segments. 

Therefore, 

With some assumptions, you can think of the binary predictor as generating from a distribution of values from a continuous distribution that has been thresholded.  Therefore, the sensitivity of this thresholded predictor actually represents one point on the ROC curve of the true underlying continuous distribution.  Therefore the ROC curve of a binary predictor is not really appropriate.  But alas, ROC and AUC analysis is done on binary predictors and used to inform if one variable is more predictive than the other.  

A more appropriate comparison of a continuous predictor and the binary predictor may be to compare the sensitivity and specificity (or overall accuracy) of the continuous predictor given the optimal threshold versus that of the binary predictor. 

This expectation directly applies to the assignment of a half probability of success when the data are tied.  This half probability is linked to how ties are treated in the Wilcoxon rank sum test.  As much of the theory of ROC curve testing, and therefore testing of differences in AUC, is based on the theory of the Wilcoxon rank-sum test, this treatment of ties is relevant to statistical inference.  

```{r, message=FALSE}
library(cranlogs)
library(ggplot2)
library(dplyr)
```
<!-- * \proglang{Java} -->
<!-- * \pkg{plyr} -->
<!-- * \code{print("abc")} -->


In this tutorial, we will show the calculation of the AUC

# Simple Example

Here we will create a simple scenario where there is a binary predictor $X$ and a binary outcome $Y$.  
```{r create_tab}
x = c(rep(0, 52), rep(1, 32),
      rep(0, 35), rep(1, 50))
y = c(rep(0, 84), rep(1, 85))
tab = table(x, y)
tab
```

## Mathematical Proof of AUC for Single Binary Predictor
As there are only two outcomes for $X$, we can expand the probability using the law of total probability:
\begin{align}
P(X_{1} > X_{0}) &= P(X_{1} > X_{0} | X_{1} = 1) P(X_{1} = 1) \nonumber \\
&+ P(X_{1} > X_{0} | X_{1} = 0) P(X_{1} = 0) \label{eq:expand1} \\
&= P(X_{1} > X_{0} | X_{1} = 1) P(X_{1} = 1) \label{eq:expand}
\end{align}
where the second term of equation \eqref{eq:expand1} is equal to zero because $X_{0} \in \{0, 1\}$.  

Here we see that the second term of equation \eqref{eq:expand} is the sensitivity:
\begin{align*}
P(X_{1} = 1) &= P(X = 1 | Y = 1)\\
&= \frac{TP}{TP + FN} \\
&= \text{sensitivity}
\end{align*}

Here we show the first term of equation \eqref{eq:expand} is the specificity:
\begin{align*}
P(X_{1} > X_{0} | X_{1} = 1) &= P(X_{1} > X_{0} | X_{1} = 1, X_{0} =1) P(X_{0} = 1) \\
&+ P(X_{1} > X_{0} | X_{1} = 1, X_{0} =0) P(X_{0} = 0) \\
&= P(X_{1} > X_{0} | X_{1} = 1, X_{0} =0) P(X_{0} = 0) \\
&= P(X_{0} = 0) \\
&= P(X = 0 | Y = 0)\\
&= \frac{TN}{TN + FP} \\
&= \text{specificity}
\end{align*}

Therefore, we combine these two to show that equation \eqref{eq:expand} reduces to:
$$
P(X_{1} > X_{0}) = \text{specificity} * \text{sensitivity}
$$


Therefore, the true AUC should be equal to:
```{r}
sens = tab[2,2] / sum(tab[,2])
spec = tab[1,1] / sum(tab[,1])
true_auc = sens * spec
print(true_auc)
```

### Reverse Labeling 

```{r}
flip_auc = (1 - sens) * (1 - spec)
print(flip_auc)
```

```{r}
fpr = 1-spec
area_of_tri = 1/2 * sens * fpr
area_of_quad = sens * spec + 1/2 * spec * (1-sens)
auc = area_of_tri + area_of_quad
```

```{r, echo = FALSE}
n = 1000000
```

We can also show that if we use a simple sampling method, we can estimate this true AUC.  Here, the function \code{est_auc} samples `r n` random samples from $X_{1}$ and $X_{0}$, then calculates $\hat{P}(X_{1} > X_{0})$:

```{r}
est_auc = function(x, y) {
  x1 = x[y == 1]
  x0 = x[y == 0]
  n = 1000000
  c1 = sample(x1, size = n, replace = TRUE)
  c0 = sample(x0, size = n, replace = TRUE)
  true_auc = mean(c1 > c0)
  # calc_auc = true_auc + 1/2 * mean(c1 == c0)
  # c(true_auc, calc_auc)
  return(true_auc)
}
sample_est_auc = est_auc(x, y)
sample_est_auc
```



# Current Implementations

## R

### caTools Package

The \pkg{caTools} package is one of the most popular packages in R, and has analysis for doing area under the curve:

```{r}
library(caTools)
colAUC(x, y)
```


### ROCR Package

The \pkg{ROCR} package is one of the most popular packages for doing ROC analysis [@ROCR].  Using \code{prediction} and \code{performance} functions, we see that the estimated AUC is much higher than the true AUC:

```{r}
library(ROCR)
pred = prediction(x, y)
auc_est = performance(pred, "auc")
auc_est@y.values[[1]]
```

Looking at the plot for the ROC curve in ROCR, we can see why this may be:
```{r, eval = FALSE, echo = TRUE}
par(mfrow = c(1, 2))
perf = performance(pred, "tpr", "fpr")
plot(perf)
abline(a = 0, b = 1)
plot(perf, type = "s")
abline(a = 0, b = 1)
```

```{r, echo = FALSE, fig.width=10, fig.height=5}
par(mfrow = c(1, 2), oma = c(0, 0, 0, 0), mar = c(5, 4.1, 0, 0))
perf = performance(pred, "tpr", "fpr")
plot(perf)
verts = cbind(x = c(0, 1 - spec, 1 - spec), y = c(0, sens, 0))
polygon(verts, col = "orange", border = FALSE)
verts = cbind(x = c(1 - spec, 1, 1), y = c(sens, 1, sens))
polygon(verts, col = "firebrick", border = FALSE)
rect(xleft = 1 - spec, xright = 1, ybottom = 0, ytop = sens, col = "deepskyblue3", border = FALSE)
lines(perf@x.values[[1]], perf@y.values[[1]], lwd = 2)
# rect(xleft = 0, ybottom =0, xright= 1-spec, ytop = sens, col = "red")
# rect(xleft = 1 - spec, ybottom = sens, xright= 1, ytop = 1, col = "blue")
abline(a = 0, b = 1,  col = "gray",lty = "dashed")
plot(perf@x.values[[1]], perf@y.values[[1]], type = "s", ylab = "", xlab = "False positive rate")
rect(xleft = 1 - spec, xright = 1, ybottom = 0, ytop = sens, col = "deepskyblue3", border = FALSE)
lines(perf@x.values[[1]], perf@y.values[[1]], lwd = 2, type = "s")
abline(a = 0, b = 1, col = "gray", lty = "dashed")
```

Looking geometrically at the plot, we can see how 
```{r}
fpr = 1 - spec
area_of_left_tri = 1/2 * sens * fpr
area_of_top_tri = 1/2 * spec * (1 - sens)
false_auc = area_of_left_tri + true_auc + area_of_top_tri
false_auc
```

### pROC Package

The \pkg{pROC} package is one of the most popular packages for doing ROC analysis [@pROC].  Using \code{prediction} and \code{performance} functions, we see that the estimated AUC is much higher than the true AUC:


```{r}
library(pROC)
pROC_roc = pROC::roc(predictor = x, response = y)
pROC_roc[["auc"]]
```


```{r, echo = FALSE, fig.width=10, fig.height=10}
ggroc(pROC_roc, colour = "firebrick") + 
  geom_step(aes(x = specificity, y = sensitivity),
            colour = "deepskyblue3")
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
par(mfrow = c(2, 2), oma = c(0, 0, 0, 0), mar = c(5, 4.1, 0, 0))
plot.roc(pROC_roc)
plot.roc(pROC_roc, type = "s")
plot(y = pROC_roc$sensitivities,
     x = 1 - pROC_roc$specificities, type = "l")
plot(y = pROC_roc$sensitivities,
     x = 1 - pROC_roc$specificities, type = "s")
```





### fbroc Package

The \pkg{fbroc} package is one of the most popular packages for doing ROC analysis [@fbroc].  Using \code{prediction} and \code{performance} functions, we see that the estimated AUC is much higher than the true AUC:


```{r}
library(fbroc)
fb_roc_default = boot.roc(x, as.logical(y), n.boot = 1000, tie.strategy = 2)
auc_def = perf(fb_roc_default, "auc")
auc_def[["Observed.Performance"]]
fb_roc_alternative = boot.roc(x, as.logical(y), n.boot = 1000, tie.strategy = 1)
auc_alt = perf(fb_roc_alternative, "auc")
auc_alt[["Observed.Performance"]]
```



```{r, echo = FALSE}
dl = cranlogs::cran_downloads(
  # when = "last-month", 
  from = "2017-09-08",
  to = "2017-10-09",
  packages = c("pROC", "ROCR", "fbroc", "AUC",
               "PKNCA", "auRoc", "caTools", "npROCRegression", 
               "roccv", "ROC632", "correctedAUC", "cvAUC",
               "optAUC", "tpAUC"))
dl = dl %>% 
  arrange(package, date) %>% 
  group_by(package) %>%
  mutate(count = cumsum(count),
         label = last(count),
         last_date = last(date)) %>% 
  ungroup
r = range(dl$date)
r[2] = r[2] + 3
dl_text = dl %>% 
  select(package, label, last_date) %>% 
  distinct %>% 
  mutate(date = last_date + 3,
         count = label) %>% 
  arrange(desc(count), package)
dl_text = dl_text %>% head(5)
dl = dl %>% 
  filter(package %in% dl_text$package)
dl_text2 = dl_text %>% 
  mutate(count = ifelse(package == "AUC", count + 2000, count),
         count = ifelse(package == "cvAUC", count - 2000, count))
dl %>% 
  ggplot(aes(x = date, y = count, group = package)) + 
  xlim(r) +
  geom_line() + 
  geom_text(aes(label = package), data = dl_text2,
                size = 3)

dl %>% 
  filter(!package %in% c("caTools")) %>% 
  ggplot(aes(x = date, y = count, group = package)) + 
  geom_line() + 
  geom_text(
    aes(label = package), 
    data = dl_text %>% 
      filter(!package %in% c("caTools")))
```


## Stata

```{r global-options, include=FALSE}
library(haven)
df = data.frame(x,y)
haven::write_dta(data = df, path = "sample_data.dta", version = 13)

library(statamd)
statapath = statamd::stata_engine_path()
profile_do(dataset = "sample_data.dta")
```

```{r, engine = "stata", engine.path = statapath, comment = ""}
roctab x y
```

## Fawcett Example

```{r fawcett}
faw = data.frame(y = c(rep(TRUE, 6), rep(FALSE, 4)),
                 x = c(0.99999, 0.99999, 0.99993, 
                       0.99986, 0.99964, 0.99955, 
                       0.68139, 0.50961, 0.48880, 0.44951))
faw$hyp = faw$x > 0.5
pred = prediction(predictions = faw$x, labels = faw$y)
par(mfrow = c(1, 2))
perf = performance(pred, "tpr", "fpr")
plot(perf)
abline(a = 0, b = 1)
plot(perf, type = "s")
abline(a = 0, b = 1)
auc_est = performance(pred, "auc")
auc_est@y.values[[1]]
est_auc(x = faw$x, y = faw$y)
```

```{r}
library(dplyr)
fawcett = function(df) {
  L_sorted = df %>% 
    arrange(desc(x), y)
  n_sample = nrow(L_sorted)
  P = sum(L_sorted[["y"]])
  N = n_sample - P
  FP = TP = 0
  R = NULL
  f_prev = -Inf
  i = 1
  for (i in seq(n_sample)) {
    f_i = L_sorted[["x"]][i]
    if (f_i != f_prev) {
      fpr = FP/N
      tpr = TP / P
      R = rbind(R, c(fpr = fpr, tpr = tpr))
      f_prev = f_i
    }
    if (L_sorted$y[i]) {
      TP = TP + 1
    }
    if (!L_sorted$y[i]) {
      FP = FP + 1
    }  
  }
  fpr = FP/N
  tpr = TP / P
  R = rbind(R, c(fpr = fpr, tpr = tpr))
  return(R) 
}
fawcett_roc = fawcett(df)
seq_range = function(x, ...) {
  rx = range(x)
  seq(rx[1], rx[2], ...)
}
# seq_range(faw$x, )
```










```{r, engine='R', include = FALSE, eval = TRUE}
unlink("profile.do")
file.remove("sample_data.dta")
```
