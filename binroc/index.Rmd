---
author:
  - name: John Muschelli
    affiliation: Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health
    address: >
      615 N Wolfe St
      Baltimore, MD 21205
    email: \email{jmuschel@jhsph.edu}
    url: http://johnmuschelli.
title:
  formatted: "ROC and AUC in R with a Single Binary Predictor"
  # If you use tex in the formatted title, also supply version without
  plain:     "ROC and AUC in R with a Single Binary Predictor"
  # For running headers, if needed
  short:     "Single Binary Predictor ROC"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [roc, auc, area under the curve, "\\proglang{R}"]
  plain:     [roc, auc, area under the curve, R]
preamble: >
  \usepackage{amsmath}
output: rticles::jss_article
bibliography: binroc.bib
---

# Introduction

```{r, message=FALSE}
library(cranlogs)
library(ggplot2)
library(dplyr)
```
<!-- * \proglang{Java} -->
<!-- * \pkg{plyr} -->
<!-- * \code{print("abc")} -->

In many applications receiver operator characteristic (ROC) curves are used to show

In this tutorial, we will show the calculation of the AUC

# Simple Example

Here we will create a simple scenario where there is a binary predictor $X$ and a binary outcome $Y$.  
```{r create_tab}
x = c(rep(0, 52), rep(1, 32),
      rep(0, 35), rep(1, 50))
y = c(rep(0, 84), rep(1, 85))
tab = table(x, y)
tab
```

## Mathematical Proof of AUC for Single Binary Predictor
As there are only two outcomes for $X$, we can expand the probability using the law of total probability:
\begin{align}
P(X_{1} > X_{0}) &= P(X_{1} > X_{0} | X_{1} = 1) P(X_{1} = 1) \nonumber \\
&+ P(X_{1} > X_{0} | X_{1} = 0) P(X_{1} = 0) \label{eq:expand1} \\
&= P(X_{1} > X_{0} | X_{1} = 1) P(X_{1} = 1) \label{eq:expand}
\end{align}
where the second term of equation \eqref{eq:expand1} is equal to zero because $X_{0} \in \{0, 1\}$.  

Here we see that the second term of equation \eqref{eq:expand} is the sensitivity:
\begin{align*}
P(X_{1} = 1) &= P(X = 1 | Y = 1)\\
&= \frac{TP}{TP + FN} \\
&= \text{sensitivity}
\end{align*}

Here we show the first term of equation \eqref{eq:expand} is the specificity:
\begin{align*}
P(X_{1} > X_{0} | X_{1} = 1) &= P(X_{1} > X_{0} | X_{1} = 1, X_{0} =1) P(X_{0} = 1) \\
&+ P(X_{1} > X_{0} | X_{1} = 1, X_{0} =0) P(X_{0} = 0) \\
&= P(X_{1} > X_{0} | X_{1} = 1, X_{0} =0) P(X_{0} = 0) \\
&= P(X_{0} = 0) \\
&= P(X = 0 | Y = 0)\\
&= \frac{TN}{TN + FP} \\
&= \text{specificity}
\end{align*}

Therefore, we combine these two to show that equation \eqref{eq:expand} reduces to:
$$
P(X_{1} > X_{0}) = \text{specificity} * \text{sensitivity}
$$


Therefore, the true AUC should be equal to:
```{r}
sens = tab[2,2] / sum(tab[,2])
spec = tab[1,1] / sum(tab[,1])
true_auc = sens * spec
print(true_auc)
```

### Reverse Labeling 

```{r}
flip_auc = (1 - sens) * (1 - spec)
print(flip_auc)
```

```{r}
fpr = 1-spec
area_of_tri = 1/2 * sens * fpr
area_of_quad = sens * spec + 1/2 * spec * (1-sens)
auc = area_of_tri + area_of_quad
```

```{r, echo = FALSE}
n = 1000000
```

We can also show that if we use a simple sampling method, we can estimate this true AUC.  Here, the function \code{est_auc} samples `r n` random samples from $X_{1}$ and $X_{0}$, then calculates $\hat{P}(X_{1} > X_{0})$:

```{r}
est_auc = function(x, y) {
  x1 = x[y == 1]
  x0 = x[y == 0]
  n = 1000000
  c1 = sample(x1, size = n, replace = TRUE)
  c0 = sample(x0, size = n, replace = TRUE)
  mean(c1 > c0)
}
sample_est_auc = est_auc(x, y)
sample_est_auc
```



# Current Implementations

## R

### caTools Package

The \pkg{caTools} package is one of the most popular packages in R, and has analysis for doing area under the curve:

```{r}
library(caTools)
colAUC(x, y)
```


### ROCR Package

The \pkg{ROCR} package is one of the most popular packages for doing ROC analysis [@ROCR].  Using \code{prediction} and \code{performance} functions, we see that the estimated AUC is much higher than the true AUC:

```{r}
library(ROCR)
pred = prediction(x, y)
auc_est = performance(pred, "auc")
auc_est@y.values[[1]]
```

Looking at the plot for the ROC curve in ROCR, we can see why this may be:
```{r, eval = FALSE, echo = TRUE}
par(mfrow = c(1, 2))
perf = performance(pred, "tpr", "fpr")
plot(perf)
abline(a = 0, b = 1)
plot(perf, type = "s")
abline(a = 0, b = 1)
```

```{r, echo = FALSE, fig.width=10, fig.height=5}
par(mfrow = c(1, 2), oma = c(0, 0, 0, 0), mar = c(5, 4.1, 0, 0))
perf = performance(pred, "tpr", "fpr")
plot(perf)
verts = cbind(x = c(0, 1 - spec, 1 - spec), y = c(0, sens, 0))
polygon(verts, col = "orange", border = FALSE)
verts = cbind(x = c(1 - spec, 1, 1), y = c(sens, 1, sens))
polygon(verts, col = "firebrick", border = FALSE)
rect(xleft = 1 - spec, xright = 1, ybottom = 0, ytop = sens, col = "deepskyblue3", border = FALSE)
lines(perf@x.values[[1]], perf@y.values[[1]], lwd = 2)
# rect(xleft = 0, ybottom =0, xright= 1-spec, ytop = sens, col = "red")
# rect(xleft = 1 - spec, ybottom = sens, xright= 1, ytop = 1, col = "blue")
abline(a = 0, b = 1,  col = "gray",lty = "dashed")
plot(perf@x.values[[1]], perf@y.values[[1]], type = "s", ylab = "", xlab = "False positive rate")
rect(xleft = 1 - spec, xright = 1, ybottom = 0, ytop = sens, col = "deepskyblue3", border = FALSE)
lines(perf@x.values[[1]], perf@y.values[[1]], lwd = 2, type = "s")
abline(a = 0, b = 1, col = "gray", lty = "dashed")
```

Looking geometrically at the plot, we can see how 
```{r}
fpr = 1 - spec
area_of_left_tri = 1/2 * sens * fpr
area_of_top_tri = 1/2 * spec * (1 - sens)
false_auc = area_of_left_tri + true_auc + area_of_top_tri
false_auc
```

### pROC Package

The \pkg{pROC} package is one of the most popular packages for doing ROC analysis [@pROC].  Using \code{prediction} and \code{performance} functions, we see that the estimated AUC is much higher than the true AUC:


```{r}
library(pROC)
pROC_roc = pROC::roc(predictor = x, response = y)
pROC_roc[["auc"]]
```


```{r, echo = FALSE, fig.width=10, fig.height=10}
ggroc(pROC_roc, colour = "firebrick") + 
  geom_step(aes(x = specificity, y = sensitivity),
            colour = "deepskyblue3")
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
par(mfrow = c(2, 2), oma = c(0, 0, 0, 0), mar = c(5, 4.1, 0, 0))
plot.roc(pROC_roc)
plot.roc(pROC_roc, type = "s")
plot(y = pROC_roc$sensitivities,
     x = 1 - pROC_roc$specificities, type = "l")
plot(y = pROC_roc$sensitivities,
     x = 1 - pROC_roc$specificities, type = "s")
```





### fbroc Package

The \pkg{fbroc} package is one of the most popular packages for doing ROC analysis [@fbroc].  Using \code{prediction} and \code{performance} functions, we see that the estimated AUC is much higher than the true AUC:


```{r}
library(fbroc)
fb_roc_default = boot.roc(x, as.logical(y), n.boot = 1000, tie.strategy = 2)
auc_def = perf(fb_roc_default, "auc")
auc_def[["Observed.Performance"]]
fb_roc_alternative = boot.roc(x, as.logical(y), n.boot = 1000, tie.strategy = 1)
auc_alt = perf(fb_roc_alternative, "auc")
auc_alt[["Observed.Performance"]]
```



```{r, echo = FALSE}
dl = cranlogs::cran_downloads(
  # when = "last-month", 
  from = "2017-09-08",
  to = "2017-10-09",
  packages = c("pROC", "ROCR", "fbroc", "AUC",
               "PKNCA", "auRoc", "caTools", "npROCRegression", 
               "roccv", "ROC632", "correctedAUC", "cvAUC",
               "optAUC", "tpAUC"))
dl = dl %>% 
  arrange(package, date) %>% 
  group_by(package) %>%
  mutate(count = cumsum(count),
         label = last(count),
         last_date = last(date)) %>% 
  ungroup
r = range(dl$date)
r[2] = r[2] + 3
dl_text = dl %>% 
  select(package, label, last_date) %>% 
  distinct %>% 
  mutate(date = last_date + 3,
         count = label) %>% 
  arrange(desc(count), package)
dl_text = dl_text %>% head(5)
dl = dl %>% 
  filter(package %in% dl_text$package)
dl_text2 = dl_text %>% 
  mutate(count = ifelse(package == "AUC", count + 2000, count),
         count = ifelse(package == "cvAUC", count - 2000, count))
dl %>% 
  ggplot(aes(x = date, y = count, group = package)) + 
  xlim(r) +
  geom_line() + 
  geom_text(aes(label = package), data = dl_text2,
                size = 3)

dl %>% 
  filter(!package %in% c("caTools")) %>% 
  ggplot(aes(x = date, y = count, group = package)) + 
  geom_line() + 
  geom_text(
    aes(label = package), 
    data = dl_text %>% 
      filter(!package %in% c("caTools")))
```


## Stata

```{r global-options, include=FALSE}
library(haven)
df = data.frame(x,y)
haven::write_dta(data = df, path = "sample_data.dta", version = 13)

library(statamd)
statapath = statamd::stata_engine_path()
profile_do(dataset = "sample_data.dta")
```

```{r, engine = "stata", engine.path = statapath, comment = ""}
roctab x y
```














```{r, engine='R', include = FALSE, eval = TRUE}
unlink("profile.do")
file.remove("sample_data.dta")
```
