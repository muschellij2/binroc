\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Mathematical Proof of AUC for Single Binary Predictor}{3}{section.2}}
\newlabel{mathematical-proof-of-auc-for-single-binary-predictor}{{2}{3}{Mathematical Proof of AUC for Single Binary Predictor}{section.2}{}}
\newlabel{eq:expand}{{1}{4}{Mathematical Proof of AUC for Single Binary Predictor}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Simple Concrete Example}{4}{subsection.2.1}}
\newlabel{simple-concrete-example}{{2.1}{4}{Simple Concrete Example}{subsection.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A simple 2x2 table of a binary predictor (rows) versus a binary outcome (columns)\relax }}{5}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:create_tab}{{1}{5}{A simple 2x2 table of a binary predictor (rows) versus a binary outcome (columns)\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Monte Carlo Estimation of AUC}{5}{subsubsection.2.1.1}}
\newlabel{monte-carlo-estimation-of-auc}{{2.1.1}{5}{Monte Carlo Estimation of AUC}{subsubsection.2.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Geometric Argument of AUC}{5}{subsubsection.2.1.2}}
\newlabel{geometric-argument-of-auc}{{2.1.2}{5}{Geometric Argument of AUC}{subsubsection.2.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces ROC curve of the data in the simple concrete example. Here we present a standard ROC curve, with the false positive rate or $1 - \text  {specificity}$ on the x-axis and true positive rate or sensitivity on the y-axis. The dotted line represents the identity. The shaded area in panel represents the AUC for the strict definition. The additional shaded areas on panel B represent the AUC when accounting for ties. \relax }}{7}{figure.caption.2}}
\newlabel{fig:main}{{1}{7}{ROC curve of the data in the simple concrete example. Here we present a standard ROC curve, with the false positive rate or $1 - \text {specificity}$ on the x-axis and true positive rate or sensitivity on the y-axis. The dotted line represents the identity. The shaded area in panel represents the AUC for the strict definition. The additional shaded areas on panel B represent the AUC when accounting for ties. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}AUC Calculation in Statistical Software}{7}{subsection.2.2}}
\newlabel{auc-calculation-in-statistical-software}{{2.2}{7}{AUC Calculation in Statistical Software}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}AUC Calculation: Current Implementations}{8}{section.3}}
\newlabel{auc-calculation-current-implementations}{{3}{8}{AUC Calculation: Current Implementations}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}R}{8}{subsection.3.1}}
\newlabel{r}{{3.1}{8}{R}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Python}{9}{subsection.3.2}}
\newlabel{python}{{3.2}{9}{Python}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}SAS Software}{10}{subsection.3.3}}
\newlabel{sas-software}{{3.3}{10}{SAS Software}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Stata}{10}{subsection.3.4}}
\newlabel{stata}{{3.4}{10}{Stata}{subsection.3.4}{}}
\newlabel{fig:stata}{{2A}{12}{Subfigure 2A}{subfigure.2.1}{}}
\newlabel{sub@fig:stata}{{(A)}{A}{Subfigure 2A\relax }{subfigure.2.1}{}}
\newlabel{fig:python}{{2B}{12}{Subfigure 2B}{subfigure.2.2}{}}
\newlabel{sub@fig:python}{{(B)}{B}{Subfigure 2B\relax }{subfigure.2.2}{}}
\newlabel{sas}{{2C}{12}{Subfigure 2C}{subfigure.2.3}{}}
\newlabel{sub@sas}{{(C)}{C}{Subfigure 2C\relax }{subfigure.2.3}{}}
\newlabel{ROCR}{{2D}{12}{Subfigure 2D}{subfigure.2.4}{}}
\newlabel{sub@ROCR}{{(D)}{D}{Subfigure 2D\relax }{subfigure.2.4}{}}
\newlabel{pROC}{{2E}{12}{Subfigure 2E}{subfigure.2.5}{}}
\newlabel{sub@pROC}{{(E)}{E}{Subfigure 2E\relax }{subfigure.2.5}{}}
\newlabel{fbroc2}{{2F}{12}{Subfigure 2F}{subfigure.2.6}{}}
\newlabel{sub@fbroc2}{{(F)}{F}{Subfigure 2F\relax }{subfigure.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of different ROC curves for different \texttt  {R} packages, \texttt  {scikit-learn} from \texttt  {Python}, \texttt  {SAS}, and \texttt  {Stata}. Each line represents the ROC curve, which corresponds to an according area under the curve (AUC). The blue shading represents the confidence interval for the ROC curve in the {\fontseries  {b}\selectfont  fbroc} package. Also, each software represents the curve as the false positive rate versus the true positive rate, though the {\fontseries  {b}\selectfont  pROC} package calls it sensitivity and specificity (with flipped axes). Some put the identity line where others do not. Overall the difference of note as to whether the ROC curve is represented by a step or a linear function. Using the first tie strategy for ties (non-default, not shown) in {\fontseries  {b}\selectfont  fbroc} gives the same confidence interval but an ROC curve using linear interpolation.\relax }}{12}{figure.caption.3}}
\newlabel{fig:rocs}{{2}{12}{Comparison of different ROC curves for different \texttt {R} packages, \texttt {scikit-learn} from \texttt {Python}, \texttt {SAS}, and \texttt {Stata}. Each line represents the ROC curve, which corresponds to an according area under the curve (AUC). The blue shading represents the confidence interval for the ROC curve in the {\fontseries {b}\selectfont fbroc} package. Also, each software represents the curve as the false positive rate versus the true positive rate, though the {\fontseries {b}\selectfont pROC} package calls it sensitivity and specificity (with flipped axes). Some put the identity line where others do not. Overall the difference of note as to whether the ROC curve is represented by a step or a linear function. Using the first tie strategy for ties (non-default, not shown) in {\fontseries {b}\selectfont fbroc} gives the same confidence interval but an ROC curve using linear interpolation.\relax }{figure.caption.3}{}}
\newlabel{fig:fbroc1}{{3A}{13}{Subfigure 3A}{subfigure.3.1}{}}
\newlabel{sub@fig:fbroc1}{{(A)}{A}{Subfigure 3A\relax }{subfigure.3.1}{}}
\newlabel{fig:fbroc2}{{3B}{13}{Subfigure 3B}{subfigure.3.2}{}}
\newlabel{sub@fig:fbroc2}{{(B)}{B}{Subfigure 3B\relax }{subfigure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of different strategies for ties in the {\fontseries  {b}\selectfont  fbroc} package. The blue shading represents the confidence interval for the ROC curve. Overall the difference of note as to whether the ROC curve is represented by a step or a linear function. Using the first tie strategy for ties (non-default) in {\fontseries  {b}\selectfont  fbroc} gives the same confidence interval as the second strategy but an ROC curve using linear interpolation, which may give an inconsistent combination of estimate and confidence interval as {\fontseries  {b}\selectfont  fbroc} reports the AUC corresponding to the linear interpolation.\relax }}{13}{figure.caption.4}}
\newlabel{fig:fbrocs}{{3}{13}{Comparison of different strategies for ties in the {\fontseries {b}\selectfont fbroc} package. The blue shading represents the confidence interval for the ROC curve. Overall the difference of note as to whether the ROC curve is represented by a step or a linear function. Using the first tie strategy for ties (non-default) in {\fontseries {b}\selectfont fbroc} gives the same confidence interval as the second strategy but an ROC curve using linear interpolation, which may give an inconsistent combination of estimate and confidence interval as {\fontseries {b}\selectfont fbroc} reports the AUC corresponding to the linear interpolation.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Categorical Predictor Example}{13}{section.4}}
\newlabel{categorical-predictor-example}{{4}{13}{Categorical Predictor Example}{section.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces A example table of a categorical predictor (rows) versus a binary outcome (columns)\relax }}{14}{table.caption.5}}
\newlabel{tab:create_cat_tab_output}{{2}{14}{A example table of a categorical predictor (rows) versus a binary outcome (columns)\relax }{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ROC curve of a 4-level categorical variable compared to the binary predictor. Here we present the ROC curve of a categorical predictor (blue points) compared to that of the binary predictor (black line). We see that the ROC curve is identical if the linear inerpolation is used (accounting for ties). The red (dotted) and blue (dashed) lines show the ROC of the binary and categorical predictor, respectively, using the pessimistic approach. We believe this demonstrates that although there is more gradation in the categorical variable, using the standard approach provides the same AUC, though we believe these variables have different levels of information as the binary predictor cannot obtain values other than the 2 categories. \relax }}{15}{figure.caption.6}}
\newlabel{fig:maincat}{{4}{15}{ROC curve of a 4-level categorical variable compared to the binary predictor. Here we present the ROC curve of a categorical predictor (blue points) compared to that of the binary predictor (black line). We see that the ROC curve is identical if the linear inerpolation is used (accounting for ties). The red (dotted) and blue (dashed) lines show the ROC of the binary and categorical predictor, respectively, using the pessimistic approach. We believe this demonstrates that although there is more gradation in the categorical variable, using the standard approach provides the same AUC, though we believe these variables have different levels of information as the binary predictor cannot obtain values other than the 2 categories. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{16}{section.5}}
\newlabel{conclusion}{{5}{16}{Conclusion}{section.5}{}}
\bibstyle{spbasic}
\bibdata{binroc.bib}
\bibcite{reticulate}{Allaire \emph  {et\nobreakspace  {}al.}(2018)Allaire, Ushey, and Tang}
\bibcite{bamber1975area}{Bamber(1975)}
\bibcite{blumberg2016technology}{Blumberg \emph  {et\nobreakspace  {}al.}(2016)Blumberg, De\nobreakspace  {}Moraes, Liebmann, Garg, Chen, Theventhiran, and Hood}
\bibcite{budwega2016factors}{Budwega \emph  {et\nobreakspace  {}al.}(2016)Budwega, Sprengerb, De\nobreakspace  {}Vere-Tyndalld, Hagenkordd, Stippichd, and Bergera}
\bibcite{delong}{DeLong \emph  {et\nobreakspace  {}al.}(1988)DeLong, DeLong, and Clarke-Pearson}
\bibcite{jama}{E \emph  {et\nobreakspace  {}al.}(2018)E, C, K, and et\nobreakspace  {}al}
\bibcite{fawcett2006introduction}{Fawcett(2006)}
\bibcite{glaveckaite2011value}{Glaveckaite \emph  {et\nobreakspace  {}al.}(2011)Glaveckaite, Valeviciene, Palionis, Skorniakov, Celutkiene, Tamosiunas, Uzdavinys, and Laucevicius}
\bibcite{hanley1982meaning}{Hanley and McNeil(1982)}
\bibcite{hsu2014inference}{Hsu and Lieli(2014)}
\bibcite{matplotlib}{Hunter(2007)}
\bibcite{kushnir2018degree}{Kushnir \emph  {et\nobreakspace  {}al.}(2018)Kushnir, Darmon, Barad, and Gleicher}
\bibcite{mwipatayi2016durability}{Mwipatayi \emph  {et\nobreakspace  {}al.}(2016)Mwipatayi, Sharma, Daneshmand, Thomas, Vijayan, Altaf, Garbowski, Jackson, Benveniste, Denton \emph  {et\nobreakspace  {}al.}}
\bibcite{scikitlearn}{Pedregosa \emph  {et\nobreakspace  {}al.}(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}
\bibcite{pepe2009estimation}{Pepe \emph  {et\nobreakspace  {}al.}(2009)Pepe, Longton, and Janes}
\bibcite{fbroc}{Peter(2016)}
\bibcite{rcore}{{R Core Team}(2018)}
\bibcite{pROC}{Robin \emph  {et\nobreakspace  {}al.}(2011)Robin, Turck, Hainard, Tiberti, Lisacek, Sanchez, and M\IeC {\"u}ller}
\bibcite{saito2015precision}{Saito and Rehmsmeier(2015)}
\bibcite{sas}{SAS and Version(2017)}
\bibcite{shterev2018bayesian}{Shterev \emph  {et\nobreakspace  {}al.}(2018)Shterev, Dunson, Chan, and Sempowski}
\bibcite{ROCR}{Sing \emph  {et\nobreakspace  {}al.}(2005)Sing, Sander, Beerenwinkel, and Lengauer}
\bibcite{snarr2017parasternal}{Snarr \emph  {et\nobreakspace  {}al.}(2017)Snarr, Liu, Zuckerberg, Falkensammer, Nadaraj, Burstein, Ho, Gardner, Butto, Ewing \emph  {et\nobreakspace  {}al.}}
\bibcite{stata}{Stata(2013)}
\bibcite{caTools}{Tuszynski(2018)}
\bibcite{jama2}{TV \emph  {et\nobreakspace  {}al.}(2017)TV, GH, JA, S, K, and GY}
\bibcite{veltri2018deep}{Veltri \emph  {et\nobreakspace  {}al.}(2018)Veltri, Kamath, and Shehu}
\bibcite{xiong2018comparison}{Xiong \emph  {et\nobreakspace  {}al.}(2018)Xiong, Li, Yang, Wei, Hu, Wang, Zhu, Li, Cao, and Xie}
